{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miniproject 1: Image Classification\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Description\n",
    "\n",
    "One of the deepest traditions in learning about deep learning is to first [tackle the exciting problem of MNIST classification](http://deeplearning.net/tutorial/logreg.html). [The MNIST database](https://en.wikipedia.org/wiki/MNIST_database) (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that was [recently extended](https://arxiv.org/abs/1702.05373). We break with this tradition (just a little bit) and tackle first the related problem of classifying cropped, downsampled and grayscaled images of house numbers in the [The Street View House Numbers (SVHN) Dataset](http://ufldl.stanford.edu/housenumbers/).\n",
    "\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- You should have a running installation of [tensorflow](https://www.tensorflow.org/install/) and [keras](https://keras.io/).\n",
    "- You should know the concepts \"multilayer perceptron\", \"stochastic gradient descent with minibatches\", \"training and validation data\", \"overfitting\" and \"early stopping\".\n",
    "\n",
    "### What you will learn\n",
    "\n",
    "- You will learn how to define feedforward neural networks in keras and fit them to data.\n",
    "- You will be guided through a prototyping procedure for the application of deep learning to a specific domain.\n",
    "- You will get in contact with concepts discussed later in the lecture, like \"regularization\", \"batch normalization\" and \"convolutional networks\".\n",
    "- You will gain some experience on the influence of network architecture, optimizer and regularization choices on the goodness of fit.\n",
    "- You will learn to be more patient :) Some fits may take your computer quite a bit of time; run them over night.\n",
    "\n",
    "### Evaluation criteria\n",
    "\n",
    "The evaluation is (mostly) based on the figures you submit and your answer sentences. \n",
    "We will only do random tests of your code and not re-run the full notebook.\n",
    "\n",
    "### Your names\n",
    "\n",
    "Before you start, please enter your full name(s) in the field below; they are used to load the data. The variable student2 may remain empty, if you work alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T09:08:24.514461Z",
     "start_time": "2018-03-09T09:08:24.506410Z"
    }
   },
   "outputs": [],
   "source": [
    "student1 = \"SimÃ£o Sarmento\"\n",
    "SCIPER1 = 284660\n",
    "student2 = \"Dennis Gankin\"\n",
    "SCIPER2 = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some helper functions\n",
    "\n",
    "For your convenience we provide here some functions to preprocess the data and plot the results later. Simply run the following cells with `Shift-Enter`.\n",
    "\n",
    "### Dependencies and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T09:09:16.113721Z",
     "start_time": "2018-03-09T09:09:16.100520Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "# you may experiment with different subsets, \n",
    "# but make sure in the submission \n",
    "# it is generated with the correct random seed for all exercises.\n",
    "np.random.seed(SCIPER1 + SCIPER2)\n",
    "subset_of_classes = np.random.choice(range(10), 5, replace = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 6\n",
    "def plot_some_samples(x, y = [], yhat = [], select_from = [], \n",
    "                      ncols = 6, nrows = 4, xdim = 16, ydim = 16,\n",
    "                      label_mapping = range(10)):\n",
    "    \"\"\"plot some input vectors as grayscale images (optionally together with their assigned or predicted labels).\n",
    "    \n",
    "    x is an NxD - dimensional array, where D is the length of an input vector and N is the number of samples.\n",
    "    Out of the N samples, ncols x nrows indices are randomly selected from the list select_from (if it is empty, select_from becomes range(N)).\n",
    "    \n",
    "    Keyword arguments:\n",
    "    y             -- corresponding labels to plot in green below each image.\n",
    "    yhat          -- corresponding predicted labels to plot in red below each image.\n",
    "    select_from   -- list of indices from which to select the images.\n",
    "    ncols, nrows  -- number of columns and rows to plot.\n",
    "    xdim, ydim    -- number of pixels of the images in x- and y-direction.\n",
    "    label_mapping -- map labels to digits.\n",
    "    \n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(nrows, ncols)\n",
    "    if len(select_from) == 0:\n",
    "        select_from = range(x.shape[0])\n",
    "    indices = np.random.choice(select_from, size = min(ncols * nrows, len(select_from)), replace = False)\n",
    "    for i, ind in enumerate(indices):\n",
    "        thisax = ax[i//ncols,i%ncols]\n",
    "        thisax.matshow(x[ind].reshape(xdim, ydim), cmap='gray')\n",
    "        thisax.set_axis_off()\n",
    "        if len(y) != 0:\n",
    "            j = y[ind] if type(y[ind]) != np.ndarray else y[ind].argmax()\n",
    "            thisax.text(0, 0, (label_mapping[j]+1)%10, color='green', \n",
    "                                                       verticalalignment='top',\n",
    "                                                       transform=thisax.transAxes)\n",
    "        if len(yhat) != 0:\n",
    "            k = yhat[ind] if type(yhat[ind]) != np.ndarray else yhat[ind].argmax()\n",
    "            thisax.text(1, 0, (label_mapping[k]+1)%10, color='red',\n",
    "                                             verticalalignment='top',\n",
    "                                             horizontalalignment='right',\n",
    "                                             transform=thisax.transAxes)\n",
    "    return fig\n",
    "\n",
    "def prepare_standardplot(title, xlabel):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.suptitle(title)\n",
    "    ax1.set_ylabel('categorical cross entropy')\n",
    "    ax1.set_xlabel(xlabel)\n",
    "    ax1.set_yscale('log')\n",
    "    ax2.set_ylabel('accuracy [% correct]')\n",
    "    ax2.set_xlabel(xlabel)\n",
    "    return fig, ax1, ax2\n",
    "\n",
    "def finalize_standardplot(fig, ax1, ax2):\n",
    "    ax1handles, ax1labels = ax1.get_legend_handles_labels()\n",
    "    if len(ax1labels) > 0:\n",
    "        ax1.legend(ax1handles, ax1labels)\n",
    "    ax2handles, ax2labels = ax2.get_legend_handles_labels()\n",
    "    if len(ax2labels) > 0:\n",
    "        ax2.legend(ax2handles, ax2labels)\n",
    "    fig.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "\n",
    "def plot_history(history, title):\n",
    "    fig, ax1, ax2 = prepare_standardplot(title, 'epoch')\n",
    "    ax1.plot(history.history['loss'], label = \"training\")\n",
    "    ax1.plot(history.history['val_loss'], label = \"validation\")\n",
    "    ax2.plot(history.history['acc'], label = \"training\")\n",
    "    ax2.plot(history.history['val_acc'], label = \"validation\")\n",
    "    finalize_standardplot(fig, ax1, ax2)\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and preprocessing the data\n",
    "\n",
    "The data consists of RGB color images with 32x32 pixels, loaded into an array of dimension 32x32x3x(number of images). We convert them to grayscale (using [this method](https://en.wikipedia.org/wiki/SRGB#The_reverse_transformation)) and we downsample them to images of 16x16 pixels by averaging over patches of 2x2 pixels.\n",
    "\n",
    "With these preprocessing steps we obviously remove some information that could be helpful in classifying the images. But, since the processed data is much lower dimensional, the fitting procedures converge faster. This is an advantage in situations like this (or generally when prototyping), where we want to try many different things without having to wait too long for computations to finish. After having gained some experience, one may want to go back to work on the 32x32 RGB images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert RGB images x to grayscale using the formula for Y_linear in https://en.wikipedia.org/wiki/Grayscale#Colorimetric_(perceptual_luminance-preserving)_conversion_to_grayscale\n",
    "def grayscale(x):\n",
    "    x = x.astype('float32')/255\n",
    "    x = np.piecewise(x, [x <= 0.04045, x > 0.04045], \n",
    "                        [lambda x: x/12.92, lambda x: ((x + .055)/1.055)**2.4])\n",
    "    return .2126 * x[:,:,0,:] + .7152 * x[:,:,1,:]  + .07152 * x[:,:,2,:]\n",
    "\n",
    "def downsample(x):\n",
    "    return sum([x[i::2,j::2,:] for i in range(2) for j in range(2)])/4\n",
    "\n",
    "def preprocess(data):\n",
    "    gray = grayscale(data['X'])\n",
    "    downsampled = downsample(gray)\n",
    "    return (downsampled.reshape(16*16, gray.shape[2]).transpose(),\n",
    "            data['y'].flatten() - 1)\n",
    "\n",
    "\n",
    "data_train = scipy.io.loadmat('housenumbers/train_32x32.mat')\n",
    "data_test = scipy.io.loadmat('housenumbers/test_32x32.mat')\n",
    "\n",
    "x_train_all, y_train_all = preprocess(data_train)\n",
    "x_test_all, y_test_all = preprocess(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a subset of classes\n",
    "\n",
    "We further reduce the size of the dataset (and thus reduce computation time) by selecting only the 5 (out of 10 digits) in subset_of_classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_classes(x, y, classes):\n",
    "    indices = []\n",
    "    labels = []\n",
    "    count = 0\n",
    "    for c in classes:\n",
    "        tmp = np.where(y == c)[0]\n",
    "        #print('tmp : ', tmp)\n",
    "        indices.extend(tmp)\n",
    "        labels.extend(np.ones(len(tmp), dtype='uint8') * count)\n",
    "        count += 1\n",
    "    return x[indices], labels\n",
    "\n",
    "x_train, y_train = extract_classes(x_train_all, y_train_all, subset_of_classes)\n",
    "x_test, y_test = extract_classes(x_test_all, y_test_all, subset_of_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot some examples now. The green digit at the bottom left of each image indicates the corresponding label in y_test.\n",
    "For further usage of the function plot_some_samples, please have a look at its definition in the plotting section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x131fc4f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_some_samples(x_test, y_test, label_mapping = subset_of_classes);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare for fitting we transform the labels to one hot coding, i.e. for 5 classes, label 2 becomes the vector [0, 0, 1, 0, 0] (python uses 0-indexing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: No hidden layer\n",
    "\n",
    "### Description\n",
    "\n",
    "Define and fit a model without a hidden layer. \n",
    "\n",
    "1. Use the softmax activation for the output layer.\n",
    "2. Use the categorical_crossentropy loss.\n",
    "3. Add the accuracy metric to the metrics.\n",
    "4. Choose stochastic gradient descent for the optimizer.\n",
    "5. Choose a minibatch size of 128.\n",
    "6. Fit for as many epochs as needed to see no further decrease in the validation loss.\n",
    "7. Plot the output of the fitting procedure (a history object) using the function plot_history defined above.\n",
    "8. Determine the indices of all test images that are misclassified by the fitted model and plot some of them using the function \n",
    "   `plot_some_samples(x_test, y_test, yhat_test, error_indices, label_mapping = subset_of_classes)`\n",
    "\n",
    "\n",
    "Hints:\n",
    "* Read the keras docs, in particular [Getting started with the Keras Sequential model](https://keras.io/getting-started/sequential-model-guide/).\n",
    "* Have a look at the keras [examples](https://github.com/keras-team/keras/tree/master/examples), e.g. [mnist_mlp](https://github.com/keras-team/keras/blob/master/examples/mnist_mlp.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paramters Definition:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_fct = 'softmax'\n",
    "neurons = 5\n",
    "loss_fct = 'categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "optimizer = 'SGD'\n",
    "batch_size = 128\n",
    "epochs = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def singlelayer_nn(activation_fct, input_dim, loss_fct, metrics, neurons, optimizer):\n",
    "    '''\n",
    "    :param activation_fct - activation function for output layer\n",
    "    :param: input_dim - input dimension\n",
    "    :param neurons - number of neurons for output layer\n",
    "    :param loss_fct - loss function to use\n",
    "    :param optimizer - type of optimizer\n",
    "    :param metrics - measuring metrics\n",
    "    '''\n",
    "    # initialize sequential model\n",
    "    model = Sequential()\n",
    "    # add output layer\n",
    "    model.add(Dense(neurons, input_dim = input_dim, activation=activation_fct))\n",
    "    # compile model...\n",
    "    model.compile(loss= loss_fct, optimizer=optimizer, metrics=metrics)\n",
    "    # print summary\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_24 (Dense)             (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 1,285\n",
      "Trainable params: 1,285\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# creating model\n",
    "model = singlelayer_nn(activation_fct, 256, loss_fct, metrics, neurons, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitting:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model take 174.98 seconds to train\n"
     ]
    }
   ],
   "source": [
    "# starting...\n",
    "start = time.time()\n",
    "# fitting...\n",
    "history = model.fit(np.asarray(x_train), y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=0)\n",
    "# the end\n",
    "end = time.time()\n",
    "\n",
    "print('Model take %0.2f seconds to train'%(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check performance:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.5391014244958257\n",
      "Test accuracy: 0.3955056179808767\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitting Procedure Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Misclassified**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: One hidden layer, different optizimizers\n",
    "### Description\n",
    "\n",
    "Train a network with one hidden layer and compare different optimizers.\n",
    "\n",
    "1. Use one hidden layer with 64 units and the 'relu' activation. Use the [summary method](https://keras.io/models/about-keras-models/) to inspect your model.\n",
    "2. Fit the model for 50 epochs with different learning rates of stochastic gradient descent and answer the question below.\n",
    "3. Replace the stochastic gradient descent optimizer with the [Adam optimizer](https://keras.io/optimizers/#adam).\n",
    "4. Plot the learning curves of SGD with a reasonable learning rate together with the learning curves of Adam in the same figure. Take care of a reasonable labeling of the curves in the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paramters Definition:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_per_layer = ['relu','softmax']\n",
    "neurons_per_layer = [64,5]\n",
    "loss_fct = 'categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "batch_size = 128\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hiddenlayer_nn(activation_per_layer, input_dim, loss_fct, metrics, neurons_per_layer, optimizer):\n",
    "    '''\n",
    "    :param: input_dim - input dimension\n",
    "    :param neurons_per_layer - list of neurons in each layer\n",
    "    :param activation_per_layer - list of activation functions in each layer\n",
    "    :param loss_fct - loss function to use\n",
    "    :param optimizer - type of optimizer\n",
    "    :param metrics - measuring metrics\n",
    "    '''\n",
    "    # initialize sequential model\n",
    "    model = Sequential()\n",
    "    # fill layers (different for first layer -> need to specify input dim)\n",
    "    for i in range(len(neurons_per_layer)):\n",
    "        if i == 0:\n",
    "            model.add(Dense(neurons_per_layer[i], input_dim = input_dim, activation=activation_per_layer[i]))\n",
    "        else:\n",
    "            model.add(Dense(neurons_per_layer[i], activation = activation_per_layer[i]))\n",
    "    # complile model\n",
    "    model.compile(loss= loss_fct, optimizer=optimizer, metrics=metrics)\n",
    "    # print summary\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing SGD w/ different learning rates:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_27 (Dense)             (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 16,773\n",
      "Trainable params: 16,773\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test loss for lr = 0.001: 1.4892707505279712\n",
      "Test accuracy for lr = 0.001: 0.3861573033707865\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_29 (Dense)             (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 16,773\n",
      "Trainable params: 16,773\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test loss for lr = 0.01: 1.004107661868749\n",
      "Test accuracy for lr = 0.01: 0.6584269663296389\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_31 (Dense)             (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 16,773\n",
      "Trainable params: 16,773\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test loss for lr = 0.1: 0.6105801318254364\n",
      "Test accuracy for lr = 0.1: 0.8176179775548785\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "for lr in learning_rates:\n",
    "    # define sgd optimizer with specific learning rate\n",
    "    sgd = SGD(lr = lr)\n",
    "    # create model\n",
    "    model = hiddenlayer_nn(activation_per_layer, 256, loss_fct, metrics, neurons_per_layer, sgd)\n",
    "    # fit model\n",
    "    history = model.fit(np.asarray(x_train), y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=0)\n",
    "    # test model\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test loss for lr = {}: {}'.format(lr, score[0]))\n",
    "    print('Test accuracy for lr = {}: {}'.format(lr, score[1]))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What happens if the learning rate of SGD is A) very large B) very small? Please answer A) and B) with one full sentence (double click this markdown cell to edit).\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "A) For larger learning rates, more information is learned in each epoch, and therefore the acuraccy is greater increased per iteration. In the end of the 50 epochs, more information was learned. \n",
    "\n",
    "B) For a smaller learning rate, less information is learned per epoch. Thus, more epochs are required in order to learn a good model. For this specific dataset, 0.001 shows to be a too small learning rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing Adam Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_33 (Dense)             (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 16,773\n",
      "Trainable params: 16,773\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test loss for Adam Optimizer: 0.586808115008172\n",
      "Test accuracy for Adam Optimizer: 0.8385617977688822\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = hiddenlayer_nn(activation_per_layer, 256, loss_fct, metrics, neurons_per_layer, 'adam')\n",
    "# fit model\n",
    "history = model.fit(np.asarray(x_train), y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=0)\n",
    "# test model\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss for Adam Optimizer: {}'.format(score[0]))\n",
    "print('Test accuracy for Adam Optimizer: {}'.format(score[1]))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plotting...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Overfitting and early stopping with Adam\n",
    "\n",
    "### Description\n",
    "\n",
    "Run the above simulation with Adam for sufficiently many epochs (be patient!) until you see clear overfitting.\n",
    "\n",
    "1. Plot the learning curves of a fit with Adam and sufficiently many epochs and answer the questions below.\n",
    "\n",
    "A simple, but effective mean to avoid overfitting is early stopping, i.e. a fit is not run until convergence but stopped as soon as the validation error starts to increase. We will use early stopping in all subsequent exercises.\n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1**: At which epoch (approximately) does the model start to overfit? Please answer with one full sentence.\n",
    "\n",
    "**Answer**: \n",
    "\n",
    "**Question 2**: Explain the qualitative difference between the loss curves and the accuracy curves with respect to signs of overfitting. Please answer with at most 3 full sentences.\n",
    "\n",
    "**Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Model performance as a function of number of hidden neurons\n",
    "\n",
    "### Description\n",
    "\n",
    "Investigate how the best validation loss and accuracy depends on the number of hidden neurons in a single layer.\n",
    "\n",
    "1. Fit a reasonable number of models with different hidden layer size (between 10 and 1000 hidden neurons) for a fixed number of epochs well beyond the point of overfitting.\n",
    "2. Collect some statistics by fitting the same models as in 1. for multiple initial conditions. Hints: 1. If you don't reset the random seed, you get different initial conditions each time you create a new model. 2. Let your computer work while you are asleep.\n",
    "3. Plot summary statistics of the final validation loss and accuracy versus the number of hidden neurons. Hint: [boxplots](https://matplotlib.org/examples/pylab_examples/boxplot_demo.html) (also [here](https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.boxplot.html?highlight=boxplot#matplotlib.axes.Axes.boxplot)) are useful. You may also want to use the matplotlib method set_xticklabels.\n",
    "4. Plot summary statistics of the loss and accuracy for early stopping versus the number of hidden neurons.\n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Comparison to deep models\n",
    "\n",
    "### Description\n",
    "\n",
    "Instead of choosing one hidden layer (with many neurons) you experiment here with multiple hidden layers (each with not so many neurons).\n",
    "\n",
    "1. Fit models with 2, 3 and 4 hidden layers with approximately the same number of parameters as a network with one hidden layer of 100 neurons. Hint: Calculate the number of parameters in a network with input dimensionality N_in, K hidden layers with N_h units, one output layer with N_out dimensions and solve for N_h. Confirm you result with the keras method model.summary().\n",
    "2. Run each model multiple times with different initial conditions and plot summary statistics of the best validation loss and accuracy versus the number of hidden layers.\n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Tricks (regularization, batch normalization, dropout)\n",
    "\n",
    "### Description\n",
    "\n",
    "Overfitting can also be counteracted with regularization and dropout. Batch normalization is supposed to mainly decrease convergence time.\n",
    "\n",
    "1. Try to improve the best validation scores of the model with 1 layer and 100 hidden neurons and the model with 4 hidden layers. Experiment with batch_normalization layers, dropout layers and l1- and l2-regularization on weights (kernels) and biases.\n",
    "2. After you have found good settings, plot for both models the learning curves of the naive model you fitted in the previous exercises together with the learning curves of the current version.\n",
    "3. For proper comparison, plot also the learning curves of the two current models in a third figure.\n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7: Convolutional networks\n",
    "\n",
    "### Description\n",
    "\n",
    "Convolutional neural networks have an inductive bias that is well adapted to image classification.\n",
    "\n",
    "1. Design a convolutional neural network, play with the parameters and fit it. Hint: You may get valuable inspiration from the keras [examples](https://github.com/keras-team/keras/tree/master/examples), e.g. [mnist_cnn](https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py).\n",
    "2. Plot the learning curves of the convolutional neural network together with the so far best performing model.\n",
    "\n",
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
